---
title: 'Deploying Next.js on Google Cloud Run with Cloud SQL — The Gotchas Nobody Tells You'
date: '2026-02-22'
lastmod: '2026-02-22'
tags: ['nextjs', 'gcp', 'cloud-run', 'cloud-sql', 'devops']
authors: ['pranavbhatkar']
summary: 'I spent hours debugging connection timeouts, permission errors, and socket failures deploying a Next.js app with Drizzle ORM to Cloud Run + Cloud SQL. Here is everything I learned so you do not have to.'
thumbnail: '/static/images/twitter-card.png'
images: ['/static/images/twitter-card.png']
layout: PostLayout
draft: true
---

# The Problem

I was deploying a Next.js 16 app (with Drizzle ORM and PostgreSQL) to Google Cloud Run. The database was on Cloud SQL. Sounds simple right? It's just a Postgres connection.

Except it wasn't. I hit **four different errors** in a row, each one more confusing than the last. The app worked perfectly locally but on Cloud Run every single database query failed with cryptic timeout errors.

If you're deploying Next.js (or any Node.js app) on Cloud Run with Cloud SQL, this post will save you hours.

## The Stack

Quick context on what I was working with:

- **Next.js 16** (App Router, `output: "standalone"`)
- **Drizzle ORM** with `node-postgres` (`pg` driver)
- **Cloud SQL** (PostgreSQL) — in a different GCP project than Cloud Run
- **Cloud Run** — containerized with Docker
- **Better Auth** for authentication (this was the first query that failed)

---

# Error 1: Connection Terminated Due to Timeout

This was the first error I saw after deploying to Cloud Run:

```bash
[cause]: Error: Connection terminated due to connection timeout
    [cause]: Error: Connection terminated unexpectedly
```

The query was a simple `SELECT` on the users table. Works locally, dies on Cloud Run.

## Why This Happens

Cloud Run is serverless. Instances spin up and down. The network between Cloud Run and Cloud SQL is not a persistent LAN connection — it goes through VPC connectors or the Cloud SQL Auth Proxy. TCP connections go stale silently.

The default `pg` Pool config doesn't account for this:

```typescript
// This is what most tutorials show you. It WILL break on Cloud Run.
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 5000,
})
```

Three problems here:

1. **No TCP keepalive** — Cloud Run's network kills idle connections without telling the client. Your pool holds dead connections and tries to use them. Timeout.
2. **`max: 20` is way too high** — each Cloud Run instance creates its own pool. If you have 10 instances, that's 200 connections to Cloud SQL. The default Cloud SQL instance has a limit of ~100.
3. **5 second timeout** — cold starts on Cloud Run can take longer, especially through a VPC connector.

## The Fix

```typescript
// src/db/index.ts
import { drizzle } from "drizzle-orm/node-postgres"
import { Pool } from "pg"
import * as schema from "./schema"

const globalForDb = globalThis as unknown as {
  pool: Pool | undefined
}

function createPool(): Pool {
  const unixSocket = process.env.INSTANCE_UNIX_SOCKET

  const poolConfig = {
    ...(unixSocket
      ? {
          user: process.env.DB_USER,
          password: process.env.DB_PASS,
          database: process.env.DB_NAME,
          host: unixSocket,
        }
      : {
          connectionString: process.env.DATABASE_URL,
        }),
    // Keep pool small for serverless
    max: 5,
    min: 0,
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 10000,
    // THIS IS THE KEY — detect dead connections
    keepAlive: true,
    keepAliveInitialDelayMillis: 10000,
    // Let Cloud Run shut down cleanly
    allowExitOnIdle: true,
  }

  const newPool = new Pool(poolConfig)

  newPool.on("error", (err) => {
    console.error("[DB Pool] Unexpected error on idle client:", err.message)
  })

  return newPool
}

const pool = globalForDb.pool ?? createPool()
globalForDb.pool = pool

export const db = drizzle(pool, { schema })
```

The critical settings:

| Setting | Value | Why |
|---|---|---|
| `max` | 5 | Prevents connection exhaustion across instances |
| `keepAlive` | true | Sends TCP probes to detect dead connections |
| `keepAliveInitialDelayMillis` | 10000 | Start probing after 10s idle |
| `connectionTimeoutMillis` | 10000 | More time for cold starts |
| `allowExitOnIdle` | true | Clean shutdown on scale-down |

The pool error handler is also important — without it, a stale connection error will crash your entire process as an unhandled rejection.

---

# Error 2: ENOENT on Unix Socket

After fixing the pool config, I deployed again and got:

```bash
Error: connect ENOENT /cloudsql/project:region:instance/.s.PGSQL.5432
```

`ENOENT` means the file doesn't exist. The app is trying to connect via a Unix socket but the socket file isn't there.

## Why This Happens

Cloud Run connects to Cloud SQL via the **Cloud SQL Auth Proxy**, which runs as a sidecar container. This proxy creates a Unix socket at `/cloudsql/INSTANCE_CONNECTION_NAME`. But you have to **explicitly add the Cloud SQL connection** to your Cloud Run service — it doesn't happen automatically just because you set `DATABASE_URL` with a socket path.

If your `DATABASE_URL` looks like this:

```bash
postgresql://user:pass@/dbname?host=/cloudsql/project:region:instance
```

...but you haven't added the Cloud SQL connection to Cloud Run, the proxy isn't running and the socket doesn't exist.

## The Fix

Two options:

**Option A: Add the Cloud SQL connection to Cloud Run**

In the Cloud Run console: go to your service, click **Edit & Deploy New Revision**, go to the **Connections** tab, and under **Cloud SQL connections**, click **Add connection** and select your Cloud SQL instance.

Or via CLI:

```bash
gcloud run deploy your-service \
  --add-cloudsql-instances=project:region:instance \
  --set-env-vars="INSTANCE_UNIX_SOCKET=/cloudsql/project:region:instance" \
  --set-env-vars="DB_USER=your_user" \
  --set-env-vars="DB_PASS=your_password" \
  --set-env-vars="DB_NAME=your_database"
```

**Option B: Use private IP instead**

If you've set up a VPC connector, use a TCP connection string instead:

```bash
DATABASE_URL="postgresql://user:pass@PRIVATE_IP:5432/dbname"
```

Don't set `INSTANCE_UNIX_SOCKET` in this case.

> **Note:** Google recommends the Unix socket approach (Option A). It is more reliable, does not require VPC configuration, and the Auth Proxy handles authentication automatically.

---

# Error 3: 403 Not Authorized

Next error after adding the Cloud SQL connection:

```bash
could not resolve instance version: failed to get instance:
refresh error: failed to get instance metadata
(connection name = "project:region:instance"):
googleapi: Error 403: NOT_AUTHORIZED
Possibly missing permission cloudsql.instances.get
```

## Why This Happens

This one is sneaky. My Cloud Run service was in **project A** but the Cloud SQL instance was in **project B**. The Cloud SQL Auth Proxy runs using the Cloud Run service account, and that service account doesn't have permission to access Cloud SQL in a different project.

Even if you're in the same project, the default Compute Engine service account might not have the `cloudsql.client` role.

## The Fix

```bash
# Find your Cloud Run service account
gcloud run services describe YOUR_SERVICE \
  --region=YOUR_REGION \
  --format="value(spec.template.spec.serviceAccountName)"

# If empty, get the default compute SA
gcloud iam service-accounts list \
  --filter="email ~ compute@developer" \
  --format="value(email)"

# Grant Cloud SQL Client role
# If same project:
gcloud projects add-iam-policy-binding YOUR_PROJECT \
  --member="serviceAccount:SA_EMAIL" \
  --role="roles/cloudsql.client"

# If cross-project (Cloud SQL in different project):
gcloud projects add-iam-policy-binding CLOUD_SQL_PROJECT \
  --member="serviceAccount:SA_EMAIL" \
  --role="roles/cloudsql.client"
```

> **Warning:** If your Cloud Run and Cloud SQL are in different projects, you MUST grant the role in the Cloud SQL project, not the Cloud Run project. This is easy to miss.

---

# Error 4: Password Authentication Failed

Almost there. Next error:

```bash
error: password authentication failed for user "myuser"
severity: 'FATAL',
code: '28P01',
```

This one is straightforward — wrong password in the environment variable. But I'm including it because it's easy to miss when you're juggling multiple env vars.

## The Fix

Double-check your `DB_PASS` (or the password in `DATABASE_URL`) matches what's set in Cloud SQL. You can reset it via:

```bash
gcloud sql users set-password YOUR_USER \
  --instance=YOUR_INSTANCE \
  --password=YOUR_NEW_PASSWORD
```

---

# Error 5: Column Does Not Exist

The connection finally works! But now:

```bash
error: column "credit_balance" does not exist
code: '42703',
```

## Why This Happens

I was connecting to a new Cloud SQL database that didn't have the latest schema. The Drizzle schema in code had columns that didn't exist in the database yet.

## The Fix

Run your migrations against the Cloud SQL database. You need the Cloud SQL Auth Proxy running locally:

```bash
# Terminal 1: Start the proxy
cloud-sql-proxy project:region:instance --port=5432

# Terminal 2: Generate + run migrations
pnpm db:generate && pnpm db:migrate
```

> **Note:** `db:generate` compares your Drizzle schema against the last snapshot and creates migration SQL files. `db:migrate` applies those files to the database. If `db:generate` says "nothing to migrate" but the database is still missing columns, you just need `db:migrate` — the migration files already exist but have not been applied to this database yet.

---

# The Complete Setup Checklist

Here's the full checklist for deploying Next.js + Drizzle on Cloud Run with Cloud SQL:

**Code changes:**
- `next.config.ts` — add `output: "standalone"`
- `src/db/index.ts` — pool config with `keepAlive`, low `max`, Unix socket support
- `Dockerfile` — standard Next.js standalone Dockerfile

**GCP setup:**
- Cloud SQL instance created with a database and user
- Cloud Run service deployed with `--add-cloudsql-instances`
- Service account has `roles/cloudsql.client` on the Cloud SQL project
- Environment variables set: `INSTANCE_UNIX_SOCKET`, `DB_USER`, `DB_PASS`, `DB_NAME`

**Database:**
- Cloud SQL Auth Proxy installed locally (`brew install cloud-sql-proxy`)
- Migrations run against Cloud SQL via the proxy

**Verification:**
- Auth flow works (this is usually the first query to hit the DB)
- No timeout errors in Cloud Run logs
- Pool errors are logged, not crashing

---

# Key Takeaways

1. **Always set `keepAlive: true`** on your `pg` Pool when running on Cloud Run. This is the single most important setting. Without it, idle connections die silently and your queries hang until they timeout.

2. **Keep `max` low** (3-5). Cloud Run creates a new pool per instance. 20 connections per instance multiplied by auto-scaling = connection exhaustion.

3. **Use Unix sockets**, not TCP. Add the Cloud SQL connection to your Cloud Run service. It's simpler and more reliable than setting up VPC connectors for private IP.

4. **Check IAM permissions** for cross-project setups. The Cloud Run service account needs `roles/cloudsql.client` in the project where Cloud SQL lives.

5. **Run migrations via the Cloud SQL Auth Proxy** locally. Don't try to run them from within Cloud Run.

6. **Add a pool error handler.** A stale connection throwing an error on an idle client will crash your Node.js process as an unhandled promise rejection if you don't catch it.

These aren't in any tutorial I found. I figured them out by hitting each error one by one. Hopefully this saves you from doing the same.
